# -*- coding: utf-8 -*-
"""T5_UTILS.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GPOExyyvZ6lKKM-PgBKBFS4NlhO60bks

# **Importing Depedencies**
"""



import pandas as pd
import ast
import transformers
from datetime import datetime
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import numpy as np
from datasets import load_dataset, Dataset, concatenate_datasets
import torch
import sklearn
import evaluate
from evaluate import evaluator
# import wandb
from transformers import T5Tokenizer, T5ForConditionalGeneration, DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments, DefaultDataCollator, pipeline
from huggingface_hub import login
login()
import nltk
from nltk.tokenize import sent_tokenize
nltk.download("punkt")


def tokenizer_and_metrics(Model, metric):
  tokenizer = T5Tokenizer.from_pretrained(Model)
  metrics = evaluate.load(metric)
  return tokenizer, metrics

tokenizer = None
metric = None

"""# **Loading Dataset**


"""

def get_data(DataSet, split = "train"):
  if DataSet.startswith("/"):
    if DataSet.endswith("csv"):
      data = pd.read_csv(DataSet)
    elif DataSet.endswith("json"):
      data = pd.read_json(DataSet)
    elif DataSet.endswith("pkl"):
      data = pd.read_pickle(DataSet)
  else:
    try:
      data = load_dataset(DataSet, split=split)
    except:
      data = load_dataset(DataSet)
  return data


def prepare_data(data, split, Q_col, A_col, prefix):
    Data = get_data(data, split)  # Assuming get_data returns a list
    try:
      df = pd.DataFrame(Data, columns=[Q_col, A_col, prefix])
    except:
      df = pd.DataFrame(Data, columns=[Q_col, A_col, prefix])
    try:
      Question = df[prefix] + " : " + df[Q_col].astype(str)
      Answer = df[A_col]
    except:
      Question = prefix + " : " + df[Q_col].astype(str)
      Answer = df[A_col]
    return Dataset.from_pandas(pd.DataFrame({"input_text": Question, "target_text": Answer}))

def identify_max_lengths(train_data, eval_data, model = "google/flan-t5-base"):
  tokenizer = T5Tokenizer.from_pretrained(model)
  # The maximum total input sequence length after tokenization. 
  # Sequences longer than this will be truncated, sequences shorter will be padded.
  tokenized_inputs = concatenate_datasets([train_data, eval_data]).map(lambda x: tokenizer(x["input_text"], truncation=True), batched=True, remove_columns=['input_text', 'target_text'])
  max_source_length = max([len(x) for x in tokenized_inputs["input_ids"]])

  # The maximum total sequence length for target text after tokenization. 
  # Sequences longer than this will be truncated, sequences shorter will be padded."
  tokenized_targets = concatenate_datasets([train_data, eval_data]).map(lambda x: tokenizer(x["target_text"], truncation=True), batched=True, remove_columns=['input_text', 'target_text'])
  max_target_length = max([len(x) for x in tokenized_targets["input_ids"]])

  return max_source_length, max_target_length



def preprocess_function(sample, max_input_length = 512, max_target_length = 512, padding="max_length", model = "google/flan-t5-base"):

    tokenizer = T5Tokenizer.from_pretrained(model)
    # tokenize inputs
    model_inputs = tokenizer(sample["input_text"], max_length=max_input_length, padding=padding, truncation=True)

    # Tokenize targets with the `text_target` keyword argument
    labels = tokenizer(text_target=sample["target_text"], max_length=max_target_length, padding=padding, truncation=True)

    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore
    # padding in the loss.
    if padding == "max_length":
        labels["input_ids"] = [
            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels["input_ids"]
        ]

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs



def preprocessed_data(train_data, eval_data, max_input_length, max_target_length, padding = "max_length", model = "google/flan-t5-base"):
  train_data_tokenized = train_data.map(preprocess_function, batched=True, remove_columns=["input_text", "target_text"], fn_kwargs = {
                        "max_input_length" : max_input_length, "max_target_length" : max_target_length,
                        "padding" : padding, "model" : model
  })

  eval_data_tokenized = eval_data.map(preprocess_function, batched=True, remove_columns=["input_text", "target_text"], fn_kwargs = {
                        "max_input_length" : max_input_length, "max_target_length" : max_target_length,
                        "padding" : padding, "model" : model
  })
  return train_data_tokenized, eval_data_tokenized


# optim (str or training_args.OptimizerNames, optional, defaults to "adamw_torch")  The optimizer to use: adamw_hf, adamw_torch, adamw_torch_fused, adamw_apex_fused, adamw_anyprecision or adafactor.

# helper function to postprocess text
def postprocess_text(preds, labels):
    preds = [pred.strip() for pred in preds]
    labels = [label.strip() for label in labels]
    # rougeLSum expects newline after each sentence
    preds = ["\n".join(sent_tokenize(pred)) for pred in preds]
    labels = ["\n".join(sent_tokenize(label)) for label in labels]
    return preds, labels


def compute_metrics(eval_preds):
    preds, labels = eval_preds
    if isinstance(preds, tuple):
        preds = preds[0]
    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True) 
    # Replace -100 in the labels as we can't decode them.
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    # Some simple post-processing
    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)
    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)
    result = {k: round(v * 100, 4) for k, v in result.items()}
    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]
    result["gen_len"] = np.mean(prediction_lens)
    return result


def preprocess_logits_for_metrics(logits, labels):
    """
    Original Trainer may have a memory leak.
    This is a workaround to avoid storing too many tensors that are not needed.
    """
    pred_ids = torch.argmax(logits[0], dim=-1)
    return pred_ids, labels



def train_model(train_data, eval_data, project = "Finetunning-t5", base_model_name = "T5", model = "t5-base",
          max_steps = 100, per_device_train_batch_size = 4, per_device_eval_batch_size = 4, save_steps = 50,
          eval_steps = 50, logging_steps = 50, checkpoint_saved = 100, optimizer = "adamw_torch", learning_rate = 2.5e-5,
          collator = "default"):
  
  project = project
  base_model_name = base_model_name
  run_name = base_model_name + "-" + project
  output_dir = "/content/" + run_name
  tokenizer = T5Tokenizer.from_pretrained(model)
  model = T5ForConditionalGeneration.from_pretrained(model)

  if collator == "seq2seq":
    data_collator = DataCollatorForSeq2Seq(
    tokenizer,
    model=model,
    label_pad_token_id= -100,
    pad_to_multiple_of=8
)
  elif collator == "default":
    data_collator = DefaultDataCollator()

  
  trainer = transformers.Trainer(
      model=model,
      train_dataset=train_data,
      eval_dataset=eval_data,
      tokenizer=tokenizer,
      compute_metrics=compute_metrics,
      preprocess_logits_for_metrics= preprocess_logits_for_metrics,

      args=transformers.TrainingArguments(
          output_dir=output_dir,
          warmup_steps=10,
          per_device_train_batch_size=per_device_train_batch_size,
          per_device_eval_batch_size=per_device_eval_batch_size,
          gradient_accumulation_steps=1,
          gradient_checkpointing=True,
          max_steps=max_steps,
          learning_rate=learning_rate, # Want a small lr for finetuning
          optim = optimizer,
          fp16_full_eval=True,
          logging_steps=logging_steps,              # When to start reporting loss
          logging_dir="./logs",        # Directory for storing logs
          save_strategy="steps",       # Save the model checkpoint every logging step
          save_steps=save_steps,                # Save checkpoints every 50 steps
          evaluation_strategy="steps", # Evaluate the model every logging step
          eval_steps=eval_steps,               # Evaluate and save checkpoints every 50 steps
          do_eval=True,               # Perform evaluation at the end of training
          report_to="wandb",
          run_name=f"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}"          # Name of the W&B run (optional)
      ),
      data_collator = data_collator,
  )

  model.config.use_cache = False  # silence the warnings. Please re-enable for inference!

  trainer.train()

  model.save_pretrained(f"/content/T5-Finetunning-t5/checkpoint-{checkpoint_saved}")
  tokenizer.save_pretrained(f"/content/T5-Finetunning-t5/checkpoint-{checkpoint_saved}") 
# train(train_data_tokenized, eval_data_tokenized, project = "Finetunning-t5", base_model_name = "T5", model = "google/flan-t5-small",
#           max_steps = 50, per_device_train_batch_size = 4, per_device_eval_batch_size = 4, save_steps = 25, eval_steps = 100, logging_steps = 100)


def inference(task, prompt, fined_tuned_path = "/content/T5-Finetunning-t5/checkpoint-100"):
  tokenizer = T5Tokenizer.from_pretrained(fined_tuned_path)
  model = T5ForConditionalGeneration.from_pretrained(fined_tuned_path)
  model.config.use_cache = True


  input_ids = tokenizer(task + " : " + prompt, return_tensors="pt", max_length=512, truncation=True).input_ids
  outputs = model.generate(input_ids, max_length=512)
  return tokenizer.decode(outputs[0], skip_special_tokens=True)



def computing_metrics_for_test(task, model, metrics, test_data):
  pipe = pipeline(task=task, model=model, device=0)

  metric = evaluate.load(metrics, use_stemmer=True, use_aggregator=True)

  task_evaluator = evaluator(task) 

  results_for_finetuned = task_evaluator.compute(
    model_or_pipeline=pipe, data=test_data, metric=metric,
    input_column="input_text", label_column="target_text"
                       )
  
  # Format Rouge metrics as percentages with two decimal places
  rouge1_percent = "{:.2%}".format(results_for_finetuned['rouge1'])
  rouge2_percent = "{:.2%}".format(results_for_finetuned['rouge2'])
  rougeL_percent = "{:.2%}".format(results_for_finetuned['rougeL'])
  rougeLsum_percent = "{:.2%}".format(results_for_finetuned['rougeLsum'])

  # Format other three values with two decimal places
  total_time = "{:.2f}".format(results_for_finetuned['total_time_in_seconds'])
  samples_per_second = "{:.2f}".format(results_for_finetuned['samples_per_second'])
  latency = "{:.2f}".format(results_for_finetuned['latency_in_seconds'])

  # Create a new dictionary with formatted values
  formatted_results = {
      'rouge1': rouge1_percent,
      'rouge2': rouge2_percent,
      'rougeL': rougeL_percent,
      'rougeLsum': rougeLsum_percent,
      'total_time_in_seconds': total_time,
      'samples_per_second': samples_per_second,
      'latency_in_seconds': latency
  }

  return formatted_results